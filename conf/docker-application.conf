# This is the main configuration file for the application.
# ~~~~~

### DEV CONFIGURATION - you should update this before deployment.

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
play.http.secret.key="%APPLICATION_SECRET%"

akka.http.server.request-timeout = 120 seconds


# The application languages
# ~~~~~
application.langs="en"

# Global object class
# ~~~~~
# Define the Global object class for this application.
# Default to Global in the root package.
# application.global=Global

# Router
# ~~~~~
# Define the Router object to use for this application.
# This router will be looked up first when the application is starting up,
# so make sure this is the entry point.
# Furthermore, it's assumed your route file is named properly.
# So for an application router like `my.application.Router`,
# you may need to define a router file `conf/my.application.routes`.
# Default to Routes in the root package (and conf/routes)
# application.router=my.application.Routes

play.filters {

  # Enabled filters are run automatically against Play.
  # CSRFFilter, AllowedHostFilters, and SecurityHeadersFilters are enabled by default.

  enabled += helpers.AllowCORSFilter

  hosts {
    allowed = [".elb.amazonaws.com","localhost:9000"]
  }

  # Disabled filters remove elements from the enabled list.
  disabled += play.filters.headers.SecurityHeadersFilter #temporarily disabled, pending testing in the frontend
  disabled += play.filters.hosts.AllowedHostsFilter #at present, enabling this breaks the tests
  disabled += play.filters.csrf.CSRFFilter #temporarily disabled until CSRF implemented in the frontend
}

# Database configuration
# ~~~~~
# You can declare as many datasources as you want.
# By convention, the default datasource is named `default`
#
play.modules.disabled += "play.api.db.DBModule" //needed to make Slick work
// can now set DATABASE_URL to a JDBC URI to access database with this
slick.dbs.default.profile="slick.jdbc.PostgresProfile$"
//slick.dbs.default.db.dataSourceClass = "slick.jdbc.DatabaseUrlDataSource"
//slick.dbs.default.db.properties.driver = "org.postgresql.Driver"
//slick.dbs.default.db.url="jdbc:postgresql://database.projectlocker/projectlocker"
//slick.dbs.default.db.properties.url="jdbc:postgresql://database.projectlocker/projectlocker"
//slick.dbs.default.db.properties.user="projectlocker"
//slick.dbs.default.db.properties.password="projectlocker"
//slick.dbs.default.db.connectionTimeout=10s
//slick.dbs.default.db.database="projectlocker"
//slick.dbs.default.db.numThreads=1
//slick.dbs.default.db.connectionTestQuery="/*ping*/ select 1"
slick.dbs.default.db {
  dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
  properties = {
    serverName="database"
    serverName = ${?DB_HOST}
    portNumber = "5432"
    databaseName = "projectlocker"
    databaseName = ${?DB_NAME}
    user = "projectlocker"
    user = ${?DB_USER}
    password = "projectlocker"
    password = ${?DB_PASSWD}
  }
  numThreads = 10
}
logger.scala.slick=DEBUG

play.evolutions.db.default.autoApply=true
play.evolutions.db.default.autoApplyDowns=true

play.http.parser.maxMemoryBuffer=512K
play.http.parser.maxDiskBuffer=419430400

ldap {
  ldapProtocol = "none"
  ldapProtocol = ${?LDAP_PROTOCOL}
  ldapUseKeystore = true
  ldapPort = 636
  ldapPort = ${?LDAP_PORT}
  ldapHost0 = "localhost"
  ldapHost0 = ${?LDAP_HOST0}
  ldapHost1 = "localhost"
  ldapHost1 = ${?LDAP_HOST1}
  bindDN = "aduser"
  bindDN = ${?LDAP_BIND_DN}
  bindPass = "adpassword"
  bindPass = ${?LDAP_BIND_PASSWORD}
  poolSize = 3
  roleBaseDN = "DC=myorg,DC=com"
  roleBaseDN = ${?LDAP_ROLE_BASE_DN}
  userBaseDN = "DC=myorg,DC=com"
  userBaseDN = ${?LDAP_USER_BASE_DN}
  uidAttribute = "samAccountName"
  memberAttribute = "member"
  roleMemberAttribute = "memberOf"
  roleAttribute = "CN"
  trustStore = "/etc/secure/keystore.jks"
  trustStorePass = "BeanstalkToTheStars"
  trustStorePass = ${?LDAP_TRUST_STORE_PASS}
  trustStoreType = "JKS"
  ldapCacheDuration = 600
  ldapCacheDuration = ${?LDAP_CACHE_DURATION}
  acg1 = "acg-name-1"
  admin-groups = [${?LDAP_ADMIN_GROUP}]
}

# Session configuration
play.http.session = {

  # The cookie name
  cookieName = "projectlocker_session"
  cookieName = ${?COOKIE_NAME}

  # Whether the secure attribute of the cookie should be set to true, i.e. only send over https.
  # we want https in production but might not be able to use it if this is done at the LB
  secure = false

  # The max age to set on the cookie.
  # If null, the cookie expires when the user closes their browser.
  # An important thing to note, this only sets when the browser will discard the cookie.
  maxAge = null

  # Whether the HTTP only attribute of the cookie should be set to true. this prevents the cookie from being accessible
  # to client-side javascript and therefore XSS attacks
  httpOnly = true

  # The value of the SameSite attribute of the cookie. Set to null for no SameSite attribute.
  sameSite = "strict"

  # The domain to set on the session cookie
  # If null, does not set a domain on the session cookie.
  # You should change this to your deployment domain
  domain = null
  domain = ${?DEPLOYMENT_DOMAIN}
  # The session path
  # Must start with /.
  path = ${play.http.context}

  jwt {
    # The JWT signature algorithm to use on the session cookie
    # uses 'alg' https://tools.ietf.org/html/rfc7515#section-4.1.1
    signatureAlgorithm = "HS256"

    # The time after which the session is automatically invalidated.
    # Use 'exp' https://tools.ietf.org/html/rfc7519#section-4.1.4
    expiresAfter = ${play.http.session.maxAge}

    # The amount of clock skew to accept between servers when performing date checks
    # If you have NTP or roughtime synchronizing between servers, you can enhance
    # security by tightening this value.
    clockSkew = 30 seconds

    # The claim key under which all user data is stored in the JWT.
    dataClaim = "data"
  }
}


#akka cluster
akka {
  pluto-message-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"

    fork-join-executor {
      parallelism-min = 2
      parallelism-factor = 2.0
      parallelism-max = 4
    }
    throughput = 1
  }

  actor {
    provider = "cluster"

    deployment {
      /message-processor-actor {
        dispatcher = pluto-message-dispatcher
      }
    }
  }

  remote {
    log-remote-lifecycle-events = off
    artery {
      canonical {
        hostname = "127.0.0.1"
        hostname = ${?HOSTNAME}
        port = 2551
      }
    }
  }

  cluster {
    seed-nodes = [
    ]

    # auto downing is NOT safe for production deployments.
    # you may want to use it during development, read more about it in the docs.
    #
    # auto-down-unreachable-after = 10s
  }

  persistence {
    journal {
      plugin = "leveldb"

      leveldb {
        dir = "target/persistence/journal"
      }
    }

    snapshot-store {
      plugin = "akka.persistence.snapshot-store.local"
      local.dir = "target/persistence/snapshot"
    }
  }

  management {
    http {
      hostname = "127.0.0.1"
      hostname = ${?HOSTNAME}
      bind-hostname = "0.0.0.0"
      port = 8558
      bind-port = 8558
    }
    cluster.bootstrap {
      new-cluster-enabled=on
      contact-point-discovery {
        required-contact-point-nr = 0 // minimum number of nodes to bootstrap the cluster
        required-contact-point-nr = ${?REQUIRED_CONTACT_POINTS}
      }
    }
  }

  //  discovery.method = akka-dns
  //  io.dns.resolver = async-dns
  discovery.method = config
  discovery.config.services {
    projectlocker {
      endpoints = [
        {
          host = "localhost"
          port = 8558
        }
      ]
    }
  }
}

leveldb {
  dir = "target/persistence/journal"
  compaction-intervals = {}
  checksum: "off"
  class: "akka.persistence.journal.leveldb.LeveldbJournal"
  dir: "target/persistence/journal"
  fsync: "on"
  native: "on"
  plugin-dispatcher : "akka.persistence.dispatchers.default-plugin-dispatcher"
  replay-dispatcher : "akka.persistence.dispatchers.default-replay-dispatcher"
}

# Enable metrics extension in akka-cluster-metrics.
akka.extensions=["akka.cluster.metrics.ClusterMetricsExtension","akka.cluster.pubsub.DistributedPubSub"]

# Sigar native library extract location during tests.
# Note: use per-jvm-instance folder when running multiple jvm on one host.
akka.cluster.metrics.native-library-extract-folder=${user.dir}/target/native


postrun {
  scriptsPath = "postrun/scripts/"
}

pluto {
  server_url = "https://my-server"
  server_url = ${?PLUTO_URL}
  sync_enabled = "no"
  username = "myaccount"
  username = ${?PLUTO_USERNAME}
  password = "mypassword"
  username = ${?PLUTO_PASSWORD}
  sitename = "VX"
  sitename = ${?PLUTO_SITENAME}
  pageSize = 100
  resend_delay = 30 seconds
  persistence-snapshot-interval = 50
}

shared_secret = "rubbish"
shared_secret = ${?SHARED_SECRET}

external {
  allowedFrontendDomains = [
    ${?CORS_DOMAINS}
  ]
}